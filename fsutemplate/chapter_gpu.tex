\chapter{GPU Computing}

\section{OpenGL}
OpenGL is a cross-platform and open industry standard for programming two and
three dimensional graphics.\cite{OpenGL} The most common use of OpenGL is for programming on
dedicated graphics hardware. OpenGL is supported by all of the major graphics
cards makers, operating systems and game development studios.


Dedicated graphics hardware is generally a combination of special processors
and memory designed for the kind of vector processing commonly encountered in
graphics programming. One of the primary tasks in graphics programming is
rasterization. Rasterization is the process of representing a three-dimensional
scene as a set of discrete, two-dimensional pixels. While there are various
techniques for rasterization, an underlying theme is the necessity for a large
amount of independent calculations being done for each pixel. These operations
include many linear algebra and other floating point math operations useful for
two and three-dimensional geometry and effects.


As graphics hardware became more sophisticated, more control of this parallel
infrastructure has been given to the programmer. This became popular with the
advent of $shaders$, small programs which could be executed by the graphics
hardware to do more sophisticated computational operations than available
through the standard interface. Shaders were named for their primary use of
shading and lighting objects, which often times calls for the computation of
sophisticated mathematical and physical models. OpenGL provides a programming
language called GLSL for writing shaders, which allows the programer to
manipulate vertices, colors, textures, and recently geometry all in parallel.


\section{General Purpose GPU Programming}

use this citation (survey): \cite{Owens2007}

With the advances in graphics hardware, most notably with the introduction of
shaders, the GPU began to share similarities with the vector processing
architectures used in super computers for scientific simulations. Researchers
began using GPUs to compute problems suitable for a parallel vector processor
at a fraction of the cost of a super computer. The speed benefits of using a
GPU over a CPU for highly parallel problems at a low cost has generated
increasing interest in using the GPU for general purpose computing.
\toi{cite the cvt article evan talks about as well as a particle system on gpu article} 

\subsection{Architecture}
The idea behind the architecture of a Graphics Processing Unit is to have many
smaller floating point processors operate on a large amount of data in
parallel.
The way this is achieved in general is by implementing a memory heirarchy which
allows each processor to quickly operate on data which it needs. An abstraction for such a memory model is given in figure X:

\begin{figure}[!htc]
 		\centering
		\includegraphics[scale=1.0]{figures/gpu_memory.pdf}
		\label{fig:logic}
        \caption{ OpenCL Memory Heirarchy Diagram \cite{OpenCLSpec} }
\end{figure}

The figure shows four main types of memory: $global$, $constant$, $local$ and
$private$. $Global$ memory is memory available to all of the processors (known
as compute units) and is the slowest type of memory in the heirarchy.
$Constant$ memory is memory which is read-only by the compute units, and is
usually cached for fast access. $Local$ memory is specific to each compute
unit, while much faster than $global$ memory it cannot be used to communicate
between compute units. Compute units are further divided into workers, which
can be thought of as individual threads which each have their own $private$
memory. Threads are executed in batches called workgroups, and each thread in a
workgroup has access to the same bank of $local$ memory, as well as access to
all of the $global$ memory. 


Since $local$ memory access is normally at least an
order of magnitude faster than $global$ memory access, GPU computing requires
great attention to the memory usage patterns of an algorithm. The most ideal
algorithms for general purpose GPU computing are Single Instruction, Multiple
Data (SIMD) algorithms in which the same operations are carried out on
completely independent data. For these algorithms each worker can operate on
the data assigned to it without concern for the behavior, status or output of
other workers. There exist many algorithms which are not completely SIMD but
can take advantage of the parallel nature of GPU programming by utilizing the
memory heirarchy in efficient ways.


\section{OpenCL}

OpenCL is a recent open standard for a parallel computing language and runtime
API put forward by the Khronos, an industry run consortium which also controls
the OpenGL standard.\cite{OpenCL} The OpenCL specification provides for two things, the
OpenCL runtime and the OpenCL programming language. The OpenCL runtime is
designed for parallel programming in heterogeneous environments, providing
mechanisms for dealing with multiple and varied compute devices such as CPUs
and the many types and generations of GPUs.  The OpenCL programming language is
a derivative of C99, giving it familiar syntax in an otherwise unfamiliar
context for many programmers.


In order to execute programs written in OpenCL a programmer must first use the
OpenCL runtime API to setup a $context$. An OpenCL context provides the
programmer with resources available from $devices$ within that context (such as
a GPU). The programmer may send memory as well as programs to the context
through the use of a $command queue$. Commands in the command queue may be
executed synchronously or asynchronously at the programmers behest. The most
important items manipulated in the command queue are $kernels$ and $buffers$.
Kernels are the OpenCL programs which will be executed on a particular device
while buffers are the memory which the kernels can take as arguments (i.e.
arrays). Kernels are compiled at runtime by OpenCL, allowing the same code to
be run on different device architectures without any guidance from the
programmer.


\subsection{CUDA}
Several years before the advent of OpenCL, graphics card manufacturer NVIDIA created the CUDA
language and runtime for general purpose computing on NVIDIA GPUs. CUDA is much
more specialized towards GPU computing than OpenCL, and also more mature as a
programming language. CUDA provides such programming language features as
Templates as well as an integrated programming environment for writing GPU code
inside of an otherwise CPU based program. The performance of CUDA on NVIDIA
cards is generally considered to be better than OpenCL, however it cannot be
run on other platforms such as those made by AMD or Intel. Due to CUDA's
proprietary nature and lack of availability on a sizeable section of the
computing population it was not considered as an option for this project.




